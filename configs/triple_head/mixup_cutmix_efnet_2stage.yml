model_params:
  model: EfficientNew
  encoder: "efficientnet_b3"
  num_classes: [168, 11, 7]

args:
  logdir: "./logs"
  seed: 65
  check: False
  verbose: True

runner_params:
  input_key: "image"
  output_key:
    - "logit_grapheme_root"
    - "logit_vowel_diacritic"
    - "logit_consonant_diacritic"
  input_target_key:
    - "grapheme_root"
    - "vowel_diacritic"
    - "consonant_diacritic"

distributed_params:  # OPTIONAL KEYWORD, параметры для distributed training и NVIDIA Apex
  opt_level: O2

stages:
  data_params:
    batch_size: 256
    num_workers: 4
    train_csv_path: "../input/bengaliutils2/"
    train_csv_name: "train_with_fold.csv"
    data_folder: "../input/bengaliai128/train_images128/"
    train_aug_name: "cutout_aug"
    fold: 0
    processed: False

  state_params:
    main_metric: hmar
    minimize_metric: False

  criterion_params:
    _key_value: True
    ce:
      criterion: CrossEntropyLoss

  callbacks_params:

    loss_mixup:
      callback: MixupCutmixCallback
      fields: ["image"]
      input_key: ["grapheme_root", "vowel_diacritic", "consonant_diacritic"]
      output_key: ["logit_grapheme_root", "logit_vowel_diacritic", "logit_consonant_diacritic"]
      criterion_key: "ce"
      prefix: "loss"
      weight_grapheme_root: 5
      weight_vowel_diacritic: 1
      weight_consonant_diacritic: 1

    saver:
      callback: CheckpointCallback
      save_n_best: 3

    metrics:
      callback: HMacroAveragedRecall

    optimizer:
      callback: OptimizerCallback

    scheduler:
      callback: SchedulerCallback
      reduce_metric: hmar

  stage1:
    state_params:
      num_epochs: 60

    optimizer_params:
      optimizer: AdamW

    scheduler_params:
      scheduler: OneCycleLRWithWarmup
      init_lr: 5e-5
      num_steps: 55
      warmup_steps: 0
      lr_range: [5e-5, 1e-7]

    callbacks_params:
      checkpoint_loader:
        callback: CheckpointLoader
        checkpoint_path: "../input/bengaliutils2/b3_9791.pth"


